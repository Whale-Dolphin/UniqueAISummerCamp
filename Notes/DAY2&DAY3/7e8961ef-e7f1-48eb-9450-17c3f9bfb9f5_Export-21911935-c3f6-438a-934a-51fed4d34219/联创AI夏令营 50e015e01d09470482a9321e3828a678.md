# 联创AI夏令营

Created: July 11, 2023 10:09 AM
Reviewed: No

# 缺失值处理

数据预处理方法

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled.png)

## 数据质量的多维评判指标

准确性（Accuracy）
完整性 （Completeness）
一致性 （Consistency）
时效性 （Timeliness）
相关性 （Relevance）
可信性（Believability）
可解释性（Interpretability）

没有高质量的数据，就没有高质量的挖掘结果，高质量的决策必须依赖高质量的数据

## 属性

是一个数据字段，表示数据，在文献中，属性、维度（dimension）、特征（feature）、变量（variance）可以互换的使用。

一个属性的类型由该属性可能具有的值的集合决定，可以是“标称的” 、“二元的” 、“序数的” 、“数值的” 。

### 标称属性

标称属性的值是一些符号或事物的名称

标称属性赋的值不具有有序性，并且不是定量的

### 二元属性

二元属性只有0和1两个值，比如果男和女、阴性和阳性等，又称布尔属性

### 序数属性

序数属性是一种属性，其可能的值之间具有有意义的序或秩评定（ranking），但是相继值之间的差是未知的。例如优良中差等，可以用0、1、2、3、4等值来评定，但两项之间的差值不一定相等

### 数据属性

是定量的可度量的量，用整数或实数表示。可以是区间标度的或比率标度的。

在机器学习领域中的分类算法通常把属性分为离散的和连续的，每种类型可以用不同的方法处理。

#### 离散属性

分为有限可数和无限可数

有限可数是指所要标注的数据是有限个数值，而无限可数表示其有无限个可数数据

#### 连续属性

## 小概率事件

一个事件如果发生概率很小那么他在一次试验中是不存在的，在多次事件中是一定会发生的

把一次实验中不可能发生的事件称为小概率事件，一般认为发生概率小于等于0.05或0.01的概率称为小概率事件

## 描述性数据汇总

### 度量数据的中心趋势

均值、中位数、众数、中列数

### 度量数据的离散程度

四分位数、四分位极差、方差

### 截断均值

去掉高、低极值得到的均值，一般截掉最高、最低2%的数据做均值

### 中心趋势度量

对于适度倾斜（非对称的）的单峰频率曲线，可以使用以下经验公式计算众数

$$
mean-mode =3\times(mean-median)
$$

其中mean是均值，mode是是众数，median是中位数

### 离散程度度量

中间四分位数极差(IQR)： IQR = Q3 – Q1
孤立点：通常我们认为：挑出落在至少高于第三个四分位数或低于第一个四分位数 1.5×IQR 处的值

### 箱线图

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%201.png)

### 三维箱线图

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%202.png)

### 直方图

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%203.png)

### Q-Q图（分位数-分位数图）

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%204.png)

Q-Q用来判断两个分布是否相似，若两个分布相同则分布在对角线上，若相似则呈线性相关

### 散布图（直接标点，简单粗暴）

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%205.png)

一般用于双变量数据

### 局部回归曲线

为散布图添加一条平滑的曲线，以便更好观察依赖模式，一般有正相关和负相关两种相关属性

## 缺失值处理

### 缺失值处理的策略

1. 均值填补
2. 随机填补
3. 基于模型的填补

### 删除法

删除样本：当样本缺失数据较多且确实特征值的样本占比不多时可使用此方法

删除特征值：当某特征值缺失较多且该特征值对分析样本数据影响不大时可使用此方法

本质时减少数据来换取信息的完整，缺失了大量隐藏在这些被删除数据中的信息，可能会造成资源的浪费

### 均值填补法

对连续型特征采用平均值进行填补，对于离散型特征采用众数进行填补

均值填补法会导致数据过多集中在平均数或众数上而导致方差偏小

由于完全忽略特征之间的相关性，均值填补法会大大弱化特征之间的相关性

在实际应用中，一般会用一定的特征来辅助判断缺失值

### 随机填补法

通过在均值填补的加上随机值来减少均值填补法的集中性

随机填补方法包括贝叶斯方法和近似贝叶斯方法

#### 贝叶斯方法

第一步：从均匀分布 U(0, 1)中随机抽取 k−1个随机数，并进行升序排序记为{0, a_1,a_2, …,a_k−1, 1}；
第二步：对 (n−k) 个缺失值，分别从非缺失值 {f_1, f_2,… , f_k} 中以概率 a_1,a_2−a_1, …,1−a_k−1采样一个值进行填补.

### 基于模型的填补方法

基于模型的填补方法用剩下的非缺失数据做训练集，训练出一个模型来预测缺失数据

改方法需要检测模型的可信度，并且用该方法填补数据会导致特征之间相关性变大

### 哑变量填补法

该方法是将缺失值作为一种新的值来进行分析，比如说将缺失值全部命名为unknown

### knn填补法

KNN填补是一种基于相似性的缺失值填补方法，它的基本思想是找到与缺失值最相似的k个样本，然后使用这k个样本的均值或中位数来填补缺失值。具体步骤如下：

1. 对于每个缺失值，计算它与数据集中所有非缺失值的距离，可以使用欧氏距离、曼哈顿距离等距离度量方法。
2. 找到距离最近的k个样本，这些样本称为邻居。
3. 计算邻居的均值或中位数，作为缺失值的填补值。
4. 重复步骤1-3，直到所有缺失值都被填补。

KNN填补的优点是简单易懂，不需要对数据做出任何假设，而且可以处理多个变量之间的关系。但是，KNN填补的缺点是计算量较大，对于大规模数据集可能会导致计算时间过长，而且需要选择合适的k值，不同的k值可能会导致不同的填补结果。

## 数据转换与编码

### 0-1标准化

又称为离差标准化或Min-Max标准化，该方法的核心即是对数据系列作线性变换，使得处理过后数据均落在［0，1］区间内，其处理函数为：

$$
⁍
$$

若希望落在[0,1]区间里，则处理函数为：

$$
⁍
$$

把某数据集映射到某区间的方法是区间归一化

设要把数据集A映射到区间[a,b]上，则处理函数为

$$
⁍
$$

$$
⁍
$$

### 小数定标标准化

通过移动数据的小数点位置来进行标准化，小数点移动多少位取决于数据系列中的最大绝对值大小，其处理函数为：

$$
⁍
$$

### Z-Score 标准化

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%206.png)

其中$\mu$为均值，$\sigma$为方差

经过Z-Score标准化后的数据，能够直观反映每个数据点距离平均值点的标准差距离，从而理解整体数据的分布情况。均值将落在0附近，而每一个数据点离零点的距离可解释为其远离均值的标准差距离。

经过变化以后变为均值为0，方差变为1

### Logistic 标准化

Logistic标准化利用Sigmoid函数的特性，将原始数据系列转化为［0，1］之间的数。

sigmoid函数$S(x)=\frac{1}{1+e^{-x}}$

Logistic标准化方法则是利用了Sigmoid函数能够映射实数至［0，1］空间这一特性来完成数据的标准化处理。其处理函数如下：

$$
x_i^*=\frac{1}{1+e^{-x_i}}
$$

## 数据编码

对不同的特征量用不同的数字表示，存在的缺陷是可能会引出错误的序和错误的距离

### One-hot编码（独热编码）

其方法是用N位寄存器对N个转台进行编码，且在任意时候只有一位有效，其余均为0。

这样编码后用向量表示可以让各个数据有正确的距离

优点：在线性回归模型中，对名义型特征进行One-Hot编码的效果通常比数字编码的效果要好. One-Hot编码对包含离散型特征的分类模型的效果有很好的提升.

缺点：特征维度会显著变多、会增加特征之间的相关性

### 哑变量编码

对于一个包含K个取值的离散型特征，将其转换成K-1个二元特征，这种编码方法称为哑变量编码

# 逻辑回归

逻辑回归是一个监督二分类任务，他的目标是通过线性分类将样本分成两个部分，通过预测找到一个最好的权重向量使分类更加准确。

我们假设一个函数$y=w^Tx$对样本进行分类。我们可以知道在这条线以上的样本带入该函是大于0的，在这条线一下的点是小于0的，我们令大于0的部分为1，为二分类中一个部分，小于0的部分为0，为二分类样本中的另一个部分。为了保证之后的随机梯度下降中我们的优化迭代可微，我们选择对数几率函数$y=\frac{1}{1+e^{-(w^Tx+b)}}$作为替代。我个人认为这个函数还有一个好处是可以弱化在分类函数周围样本的权重从而使分类更加具有代表性。

根据一系列数学变换以后我们可以得到如下两个式子

$$
⁍

$$

$$
⁍
$$

(y1s1中间过程有点点没看懂，之后再来详细推一推QAQ)

之后我们进行极大似然估计的部分，就是来求一个函数，这个函数和模型的准确度有关，也叫做对数似然估计函数，他的值越小我们最后模型的可信度就越高

我们构建一个极大 似然估计函数

$$
L(w)=\displaystyle\prod_{i=1}^{i=k}p(x_i)\displaystyle\prod_{i=k+1}^{i=n}(1-p(x_i))
$$

化简并取对数可得

$$
⁍
$$

该函数成为平均对数似然函数，略去了中间的推到过程（中间从p(x)带入那一步没看懂QAQ，标记一下）

## 正则化

为了防止对数据的过拟合我们还需要对这个函数进行正则化处理，正则化分为两类L1正则化和L2正则化

### L1正则化

（我查了一下好像不都是这么讲的，但是我看的这篇文章是这样说的，阿巴阿巴）

L1正则化又叫做LASSO回归，相当于为模型添加了一个w服从零均值拉普拉斯分布的先验知识，拉普拉斯分布如下

$$
⁍
$$

引入先验知识以后似然函数可以修改为

$$
\begin{aligned}
L(w)&=P(y|w,x)P(w) \\
&=\displaystyle\prod_{i=1}^{N}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\displaystyle\prod_{j=1}^d\frac{1}{2b}e^{-\frac{|w_j|}{b}} \\
\end{aligned}
$$

取ln取负后得到目标函数：

$$
⁍
$$

或写作正则化系数的形式

$$
L(w, b) = \frac{1}{n} \displaystyle\sum[y_i * ln(p_i) + (1 - y_i) * log(1 - p_i)] - \lambda||w||
$$

个人理解文章里这么讲是为了让我们理解为什么要这样进行正则化并且这样的理解可以更好的帮助我们理解两种正则化之间的区别和优势和一些正则化之中的问题

### L2正则化

L2正则化又叫做Ridge 回归，相当于为模型添加了这样一个先验知识：w 服从零均值正态分布

正态分布表达式如下

$$
⁍
$$

引入先验知识后似然函数如下

$$
\begin{aligned}
L(w)&=P(y|w,x)P(w) \\
&=\displaystyle\prod_{i=1}^{N}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\displaystyle\prod_{j=1}^d\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{w_j^2}{2\sigma^2}} \\
&=\displaystyle\prod_{i=1}^{N}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{w^Tw}{2\sigma^2}} \\
\end{aligned}
$$

取ln再取负后得到目标函数

$$
J(w)=-\frac{1}{n}\displaystyle\sum_i[y_ilnp(x_i)+(1-y_i)ln(1-p(x_i))]+\frac{1}{2\sigma^2}w^Tw
$$

或写作带正则系数的形式

$$
⁍
$$

下面介绍的是几种反向传播算法，反向传播算法是机器学习中的核心算法，正是他优化了参数使得机器学习模型的准确率更高

### 随机梯度下降（SGD）

随机梯度下降（SGD）是一种简单但非常有效的方法，多用用于支持向量机、逻辑回归（LR）等凸损失函数下的线性分类器的学习。并且SGD已成功应用于文本分类和自然语言处理中经常遇到的大规模和稀疏机器学习问题。

SGD既可以用于分类计算，也可以用于回归计算。

以下是SGD的核心思想：

SGD算法是**从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。**（重点：每次迭代使用一组样本。）

```python
# SGD
x_train = np.hstack((x_train.values, np.ones((len(x_train), 1)))) 
w = np.zeros((x_train.shape[1], 1))  
lr = 0.01  
epochs = 1000  
for i in range(epochs):
    idx = np.random.randint(len(x_train))  
    y_pred = 1 / (1 + np.exp(-np.dot(x_train[idx], w)))  # sigmoid 函数
    grad = x_train[idx].reshape(-1, 1) * (y_pred - y_train[idx])  # 计算梯度
    w -= lr * grad  # 更新权重
```

以上是一个简单的SGD的实现，能通过代码和理论的结合更好理解算法