# 联创AI夏令营

Created: July 11, 2023 10:09 AM
Reviewed: No

# 缺失值处理

数据预处理方法

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled.png)

## 数据质量的多维评判指标

准确性（Accuracy）
完整性 （Completeness）
一致性 （Consistency）
时效性 （Timeliness）
相关性 （Relevance）
可信性（Believability）
可解释性（Interpretability）

没有高质量的数据，就没有高质量的挖掘结果，高质量的决策必须依赖高质量的数据

## 属性

是一个数据字段，表示数据，在文献中，属性、维度（dimension）、特征（feature）、变量（variance）可以互换的使用。

一个属性的类型由该属性可能具有的值的集合决定，可以是“标称的” 、“二元的” 、“序数的” 、“数值的” 。

### 标称属性

标称属性的值是一些符号或事物的名称

标称属性赋的值不具有有序性，并且不是定量的

### 二元属性

二元属性只有0和1两个值，比如果男和女、阴性和阳性等，又称布尔属性

### 序数属性

序数属性是一种属性，其可能的值之间具有有意义的序或秩评定（ranking），但是相继值之间的差是未知的。例如优良中差等，可以用0、1、2、3、4等值来评定，但两项之间的差值不一定相等

### 数据属性

是定量的可度量的量，用整数或实数表示。可以是区间标度的或比率标度的。

在机器学习领域中的分类算法通常把属性分为离散的和连续的，每种类型可以用不同的方法处理。

#### 离散属性

分为有限可数和无限可数

有限可数是指所要标注的数据是有限个数值，而无限可数表示其有无限个可数数据

#### 连续属性

## 小概率事件

一个事件如果发生概率很小那么他在一次试验中是不存在的，在多次事件中是一定会发生的

把一次实验中不可能发生的事件称为小概率事件，一般认为发生概率小于等于0.05或0.01的概率称为小概率事件

## 描述性数据汇总

### 度量数据的中心趋势

均值、中位数、众数、中列数

### 度量数据的离散程度

四分位数、四分位极差、方差

### 截断均值

去掉高、低极值得到的均值，一般截掉最高、最低2%的数据做均值

### 中心趋势度量

对于适度倾斜（非对称的）的单峰频率曲线，可以使用以下经验公式计算众数

$$
mean-mode =3\times(mean-median)
$$

其中mean是均值，mode是是众数，median是中位数

### 离散程度度量

中间四分位数极差(IQR)： IQR = Q3 – Q1
孤立点：通常我们认为：挑出落在至少高于第三个四分位数或低于第一个四分位数 1.5×IQR 处的值

### 箱线图

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%201.png)

### 三维箱线图

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%202.png)

### 直方图

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%203.png)

### Q-Q图（分位数-分位数图）

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%204.png)

Q-Q用来判断两个分布是否相似，若两个分布相同则分布在对角线上，若相似则呈线性相关

### 散布图（直接标点，简单粗暴）

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%205.png)

一般用于双变量数据

### 局部回归曲线

为散布图添加一条平滑的曲线，以便更好观察依赖模式，一般有正相关和负相关两种相关属性

## 缺失值处理

### 缺失值处理的策略

1. 均值填补
2. 随机填补
3. 基于模型的填补

### 删除法

删除样本：当样本缺失数据较多且确实特征值的样本占比不多时可使用此方法

删除特征值：当某特征值缺失较多且该特征值对分析样本数据影响不大时可使用此方法

本质时减少数据来换取信息的完整，缺失了大量隐藏在这些被删除数据中的信息，可能会造成资源的浪费

### 均值填补法

对连续型特征采用平均值进行填补，对于离散型特征采用众数进行填补

均值填补法会导致数据过多集中在平均数或众数上而导致方差偏小

由于完全忽略特征之间的相关性，均值填补法会大大弱化特征之间的相关性

在实际应用中，一般会用一定的特征来辅助判断缺失值

### 随机填补法

通过在均值填补的加上随机值来减少均值填补法的集中性

随机填补方法包括贝叶斯方法和近似贝叶斯方法

#### 贝叶斯方法

第一步：从均匀分布 U(0, 1)中随机抽取 k−1个随机数，并进行升序排序记为{0, a_1,a_2, …,a_k−1, 1}；
第二步：对 (n−k) 个缺失值，分别从非缺失值 {f_1, f_2,… , f_k} 中以概率 a_1,a_2−a_1, …,1−a_k−1采样一个值进行填补.

### 基于模型的填补方法

基于模型的填补方法用剩下的非缺失数据做训练集，训练出一个模型来预测缺失数据

改方法需要检测模型的可信度，并且用该方法填补数据会导致特征之间相关性变大

### 哑变量填补法

该方法是将缺失值作为一种新的值来进行分析，比如说将缺失值全部命名为unknown

### knn填补法

KNN填补是一种基于相似性的缺失值填补方法，它的基本思想是找到与缺失值最相似的k个样本，然后使用这k个样本的均值或中位数来填补缺失值。具体步骤如下：

1. 对于每个缺失值，计算它与数据集中所有非缺失值的距离，可以使用欧氏距离、曼哈顿距离等距离度量方法。
2. 找到距离最近的k个样本，这些样本称为邻居。
3. 计算邻居的均值或中位数，作为缺失值的填补值。
4. 重复步骤1-3，直到所有缺失值都被填补。

KNN填补的优点是简单易懂，不需要对数据做出任何假设，而且可以处理多个变量之间的关系。但是，KNN填补的缺点是计算量较大，对于大规模数据集可能会导致计算时间过长，而且需要选择合适的k值，不同的k值可能会导致不同的填补结果。

## 数据转换与编码

### 0-1标准化

又称为离差标准化或Min-Max标准化，该方法的核心即是对数据系列作线性变换，使得处理过后数据均落在［0，1］区间内，其处理函数为：

$$
⁍
$$

若希望落在[0,1]区间里，则处理函数为：

$$
⁍
$$

把某数据集映射到某区间的方法是区间归一化

设要把数据集A映射到区间[a,b]上，则处理函数为

$$
⁍
$$

$$
⁍
$$

### 小数定标标准化

通过移动数据的小数点位置来进行标准化，小数点移动多少位取决于数据系列中的最大绝对值大小，其处理函数为：

$$
⁍
$$

### Z-Score 标准化

![Untitled](%E8%81%94%E5%88%9BAI%E5%A4%8F%E4%BB%A4%E8%90%A5%2050e015e01d09470482a9321e3828a678/Untitled%206.png)

其中$\mu$为均值，$\sigma$为方差

经过Z-Score标准化后的数据，能够直观反映每个数据点距离平均值点的标准差距离，从而理解整体数据的分布情况。均值将落在0附近，而每一个数据点离零点的距离可解释为其远离均值的标准差距离。

经过变化以后变为均值为0，方差变为1

### Logistic 标准化

Logistic标准化利用Sigmoid函数的特性，将原始数据系列转化为［0，1］之间的数。

sigmoid函数$S(x)=\frac{1}{1+e^{-x}}$

Logistic标准化方法则是利用了Sigmoid函数能够映射实数至［0，1］空间这一特性来完成数据的标准化处理。其处理函数如下：

$$
x_i^*=\frac{1}{1+e^{-x_i}}
$$

## 数据编码

对不同的特征量用不同的数字表示，存在的缺陷是可能会引出错误的序和错误的距离

### One-hot编码（独热编码）

其方法是用N位寄存器对N个转台进行编码，且在任意时候只有一位有效，其余均为0。

这样编码后用向量表示可以让各个数据有正确的距离

优点：在线性回归模型中，对名义型特征进行One-Hot编码的效果通常比数字编码的效果要好. One-Hot编码对包含离散型特征的分类模型的效果有很好的提升.

缺点：特征维度会显著变多、会增加特征之间的相关性

### 哑变量编码

对于一个包含K个取值的离散型特征，将其转换成K-1个二元特征，这种编码方法称为哑变量编码

# 逻辑回归

逻辑回归是一个监督二分类任务，他的目标是通过线性分类将样本分成两个部分，通过预测找到一个最好的权重向量使分类更加准确。

我们假设一个函数$y=w^Tx$对样本进行分类。我们可以知道在这条线以上的样本带入该函是大于0的，在这条线一下的点是小于0的，我们令大于0的部分为1，为二分类中一个部分，小于0的部分为0，为二分类样本中的另一个部分。为了保证之后的随机梯度下降中我们的优化迭代可微，我们选择对数几率函数$y=\frac{1}{1+e^{-(w^Tx+b)}}$作为替代。我个人认为这个函数还有一个好处是可以弱化在分类函数周围样本的权重从而使分类更加具有代表性。

根据一系列数学变换以后我们可以得到如下两个式子

$$
⁍

$$

$$
⁍
$$

(y1s1中间过程有点点没看懂，之后再来详细推一推QAQ)

之后我们进行极大似然估计的部分，就是来求一个函数，这个函数和模型的准确度有关，也叫做对数似然估计函数，他的值越小我们最后模型的可信度就越高

我们构建一个极大 似然估计函数

$$
L(w)=\displaystyle\prod_{i=1}^{i=k}p(x_i)\displaystyle\prod_{i=k+1}^{i=n}(1-p(x_i))
$$

化简并取对数可得

$$
⁍
$$

该函数成为平均对数似然函数，略去了中间的推到过程（中间从p(x)带入那一步没看懂QAQ，标记一下）

## 正则化

为了防止对数据的过拟合我们还需要对这个函数进行正则化处理，正则化分为两类L1正则化和L2正则化

### L1正则化

（我查了一下好像不都是这么讲的，但是我看的这篇文章是这样说的，阿巴阿巴）

L1正则化又叫做LASSO回归，相当于为模型添加了一个w服从零均值拉普拉斯分布的先验知识，拉普拉斯分布如下

$$
⁍
$$

引入先验知识以后似然函数可以修改为

$$
\begin{aligned}
L(w)&=P(y|w,x)P(w) \\
&=\displaystyle\prod_{i=1}^{N}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\displaystyle\prod_{j=1}^d\frac{1}{2b}e^{-\frac{|w_j|}{b}} \\
\end{aligned}
$$

取ln取负后得到目标函数：

$$
⁍
$$

或写作正则化系数的形式

$$
L(w, b) = \frac{1}{n} \displaystyle\sum[y_i * ln(p_i) + (1 - y_i) * log(1 - p_i)] - \lambda||w||
$$

个人理解文章里这么讲是为了让我们理解为什么要这样进行正则化并且这样的理解可以更好的帮助我们理解两种正则化之间的区别和优势和一些正则化之中的问题

### L2正则化

L2正则化又叫做Ridge 回归，相当于为模型添加了这样一个先验知识：w 服从零均值正态分布

正态分布表达式如下

$$
⁍
$$

引入先验知识后似然函数如下

$$
\begin{aligned}
L(w)&=P(y|w,x)P(w) \\
&=\displaystyle\prod_{i=1}^{N}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\displaystyle\prod_{j=1}^d\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{w_j^2}{2\sigma^2}} \\
&=\displaystyle\prod_{i=1}^{N}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{w^Tw}{2\sigma^2}} \\
\end{aligned}
$$

取ln再取负后得到目标函数

$$
J(w)=-\frac{1}{n}\displaystyle\sum_i[y_ilnp(x_i)+(1-y_i)ln(1-p(x_i))]+\frac{1}{2\sigma^2}w^Tw
$$

或写作带正则系数的形式

$$
⁍
$$

下面介绍的是几种反向传播算法，反向传播算法是机器学习中的核心算法，正是他优化了参数使得机器学习模型的准确率更高

### 随机梯度下降（SGD）

随机梯度下降（SGD）是一种简单但非常有效的方法，多用用于支持向量机、逻辑回归（LR）等凸损失函数下的线性分类器的学习。并且SGD已成功应用于文本分类和自然语言处理中经常遇到的大规模和稀疏机器学习问题。

SGD既可以用于分类计算，也可以用于回归计算。

以下是SGD的核心思想：

SGD算法是**从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。**（重点：每次迭代使用一组样本。）

```python
# SGD
x_train = np.hstack((x_train.values, np.ones((len(x_train), 1)))) 
w = np.zeros((x_train.shape[1], 1))  
lr = 0.01  
epochs = 1000  
for i in range(epochs):
    idx = np.random.randint(len(x_train))  
    y_pred = 1 / (1 + np.exp(-np.dot(x_train[idx], w)))  # sigmoid 函数
    grad = x_train[idx].reshape(-1, 1) * (y_pred - y_train[idx])  # 计算梯度
    w -= lr * grad  # 更新权重
```

以上是一个简单的SGD的实现，能通过代码和理论的结合更好理解算法

以下是基于L1正则化的随机梯度下降优化权重参数的实现推导过程

加入正则化项之后的式子应修改为：

$$
L(w, b) = \frac{1}{n} \displaystyle\sum[y_i * ln(p_i) + (1 - y_i) * ln(1 - p_i)] - \lambda||w||
$$

将

$$
⁍

$$

$$
⁍
$$

分别带入可得：

$$
⁍
$$

对w求一阶偏导可得：

$$
\frac{\partial J(w)}{\partial w}=-\frac{1}{n}\displaystyle\sum_{i=1}^n(y_ix-\frac{x·e^{w^Tx+b}}{e^{w^Tx+b}+1})+\lambda \sum sign(w)
$$

对于一个元来说可以去掉$\sum$并且将x提出后可得：

$$
\frac{\partial J(w)}{\partial w}=-x(y_i-\frac{e^{w^Tx+b}}{e^{w^Tx+b}+1})+\lambda sign(w)
$$

注意到$\frac{e^{w^Tx+b}}{e^{w^Tx+b}+1}$可以化简为$p(y_i)$，进一步化简可得

$$
⁍
$$

最后用

$$
⁍
$$

更新权重参数即可

## 动量法

动量法是梯度下降算法的一种，它一定程度解决了SGD法收敛慢的问题，它和SGD的主要区别是更新公式不一样，动量法的更新公式是：

$$
⁍
$$

$$
⁍
$$

其中v是动量参数，一般初始化为0，g是参数的梯度值

（原理没怎么搞清楚后面再来看吧，阿巴阿巴）

## Adagrad

通常，我们在每一次更新参数时，对于所有的参数使用相同的学习率。而AdaGrad算法的思想是：每一次更新参数时（一次迭代），不同的参数使用不同的学习率。

P.S.其实随机梯度下降可以可以通过改变学习率使得训练出来的模型更准确

更新参数的算法为：

$$
⁍
$$

$$
⁍
$$

其中$\delta$一般取1e-7，用于防止分母为0

## RMSprop

这个算法也是对adagrad的一个修正，在adagrad的基础上引入了指数加权的概念，是平方梯度的更新只累积窗口大小的时间步的梯度。具体更新如下：

$$
⁍
$$

$$
⁍
$$

其中$\rho \epsilon$都是需要设置的超参数，$\delta$同上是一个小值为了防止分母为0

## Adam

Adam算法的结构如下，其算法主要是在REMSprop的基础上增加了momentum，并进行了偏差修正。如下图算法中的$m_t$可理解为momentum，$v_t$可理解为梯度变化的方差，他们分别是$g_t$的一阶和二阶矩估计。

$$
⁍
$$

$$
⁍
$$

$$
⁍
$$

$$
\hat{v}_t=\frac{v_t}{1-\beta_2^t}
$$

$$
⁍
$$

其中默认值设置*α*=0.001，*β*1=0.9，*β*2=0.999，*ε*=10−8。

（看看我今天能不能把这段的数学过程看懂QAQ，争取吧）

# SVM

po一个视频，数学部分讲的很好：

【数之道25】机器学习必经之路-SVM支持向量机的数学精华】 [https://www.bilibili.com/video/BV13r4y1z7AG/?share_source=copy_web&vd_source=af6b906242c5921735eb58823e0057d6](https://www.bilibili.com/video/BV13r4y1z7AG/?share_source=copy_web&vd_source=af6b906242c5921735eb58823e0057d6)

支持向量机解决的是二分类问题，它的核心思想是找到一个向量平面或者超平面将数据点分开，并保证该超平面在两个支持向量之间获得最大的距离，具体数学推导过程如下

我们以二维平面分类为例

设分类的决策超平面为$w_1x_1+w_2x_2+b=0$，正超平面为$w_1x_1+w_2x_2+b=1$，负超平面为$w_1x_1+w_2x_2+b=-1$，取在正超平面和负超平面上的两个支持向量点$x_n,x_m$带入正负超平面中，可以得到

$$
⁍
$$

两式相减可得

$$
\overrightarrow w ·(\overrightarrow x_m -\overrightarrow x_n)=2 \cdots (1)
$$

在决策超平面上再选两个点$x_p x_q$可得

$$
⁍
$$

相减可得

$$
⁍
$$

由向量点乘的几何意义可知，权重向量w垂直与PQ，即垂直于权重超平面

再由(1)可知

$$
||w||·2l=2
$$

其中l是支持向量点到决策超平面的距离，由此我们可以知道

$$
l=\frac{1}{||w||}
$$

我们的目标就是要求$l$的最大值

显而易见的，求最大值的约束条件就是其余所有的点都在正负超平面以外，可得约束条件为

$$
⁍
$$

又$||\overrightarrow w||=\sqrt{w_1^2+w_2^2}$，为了消去根式简化后面的计算，我们可以将求$||\overrightarrow w||$的最小值等价为求$\frac{||\overrightarrow w||^2}{2}$的最小值，设$y_i·(wx+b)-1=q_i^2$，由拉格朗日法可知

$$
L(w,b,\lambda,x)=\frac{||\overrightarrow w||^2}{2}-\sum\lambda_i(y_i·(wx+b)-1-q_i^2)
$$

分别对四个变量求导可知

$$
\overrightarrow w-\displaystyle\sum_{i=1}^S\lambda_iy_i\overrightarrow{x_i}=0 \\
-\displaystyle\sum_{i=1}^S\lambda_iy_i=0 \\
y_i·(wx+b)-1-q_i^2=0 \\
2\lambda_iy_i=0
$$

对第四个式子做$\lambda_ip_i^2=0$和第三个式子联立可知

$$
⁍
$$

这个条件也称为互补松弛条件

可知$\lambda_i\geqslant0$（推导挖个坑，到时候再来补一补）

该条件和以上四个条件就是原问题的KKT条件

## 拉格朗日对偶化

拉格朗日对偶化详细推导见后面专门的拉格朗日对偶化部分，这里只给出结论

求解上面的问题等价于求解下面的对偶问题：

$$
⁍
$$

其中$q(\lambda_i^*)<f(\overrightarrow {w^*})$则该对偶问题为弱对偶问题$q(\lambda_i^*)=f(\overrightarrow {w^*})$时该问题为强对偶问题

其中强对偶问题的必要条件为Slater条件，必要条件为KKT条件，具体的推导和Slater条件的表达式我们放在拉格朗日对偶化去讲

上述方程带入KKT条件以后可以化简如下

$$
⁍
$$

我们通过求解$\lambda_i$后可以通过$\overrightarrow w-\displaystyle\sum_{i=1}^S\lambda_iy_i\overrightarrow{x_i}=0$来求得$w$的值

根据KKT条件中的互补松弛条件也就是$\lambda_i(y_i·(\overrightarrow w ·\overrightarrow x_i+b))=0$可知，除了支持向量以外的$\lambda$值均为0，故求解$\lambda_i$时我们只需要将支持向量带入计算即可

## 核函数和核技巧

由于很多分类问题我们无法在该维度的空间进行分类，我们需要对数据进行升维变换，设维度转换函数为$T(x)$，之后我们对每个$x_i,x_j$进行升维变换，如果我们把两个向量的点乘打包运算，就得到了核函数$K(\overrightarrow x_i,\overrightarrow x_j)=T(\overrightarrow x_i)·T(\overrightarrow x_j)$，这样的转换方法我们称之为核技巧

核函数的一般表达式如下

$$
⁍
$$

### RBF(高斯核函数)

$$
⁍
$$

$\gamma$值越大各点间的相似度越小，$\gamma$值越小，各点间的相似度越大

反映到算法中也就是说或决定了后面加入的值会和训练值有多大的相似度从而得到更普适或者更精准的结论

### sigmoid核函数

## 软间隔

软间隔主要用来处理异常值，为了让对未来数据的预测更加准确

对于一个异常点a来说其损失值为（该损失值是相对于正负超平面而言的）

$$
⁍
$$

由该函数形式可知，在其不断靠近负超平面时其值不断减少，达到负超平面时为0

其实际损失函数也称之为铰链损失函数如下：

$$
\epsilon_i=max(0,1-y_i·(\overrightarrow w·\overrightarrow x_i+b))
$$

我们可以将上面的目标函数加入损失函数如下：

$$
⁍
$$

即我们要对该函数使用拉格朗日法，其中$\sigma$是损失系数，用来控制对损失的容忍度

## SMO算法（该部分是SVM代码部分的核心）

smo算法是一种优化算法主要是用来求对偶化之后的式子，它的作用是将大优化问题分解为多个小优化问题来求解，从而提高求解效率。由于上面的对偶函数的值只与支持向量的位置有关，所以我们可以只考虑两个$\lambda$值并进行同步的优化，则优化函数可以改写如下：

$$
K_{ij}=K(x_i,x_j)\\
f(x_i)=\displaystyle\sum_{j=1}^ny_j\lambda_jK_{ij}+b\\
v_i=f(x_i)-\displaystyle\sum_{j=1}^2y_j\lambda_jK_{ij}-b\\
q(\lambda_1,\lambda_2)=\lambda_1+\lambda_2-\frac{1}{2}K_{11}\lambda_1^2\frac{1}{2}K_{22}\lambda_2^2-y_1y_2K_{12}\lambda_1\lambda_2-y_1\lambda_1v_1-y_2\lambda_2v_2+C
$$

由KKT条件中的$\displaystyle\sum_{i=1}^ny_i\lambda_i=0$，再根据假设其他参数均为零我们可以设$\lambda_1y_1+\lambda_2y_2=C$，又$y_i\in\{-1,1\}$，我们可以变换得到如下等式$\lambda_1=\gamma-s\lambda_2$（$\gamma=y_1C,s=y_1y_2)$带入上面的式子可以得到只有$\lambda_2$一个参数的式子：

$$
⁍
$$

对$\lambda_2$求偏导可知

$$
\frac{\partial p(\lambda_2)}{\partial \lambda_2}=-s+1+sK_{11}\gamma-K_{11}\lambda_2-K_{22}\lambda_2+2K_{12}\lambda_2-sK_{12}\gamma+y_2v_1-y_1v_2=0
$$

由此可以得到

$$
⁍
$$

设误差项$E_i=f(x_i)-y_i$，又$\gamma=\lambda_1^{old}+s\lambda_2^{old}$，$K=K_{11}+K_{22}-2K_{12}$，上述结果可以化简为

$$
⁍
$$

又因为$y_1,y_2$只能为-1或1，所以我们需要对区域外的数据进行剪辑，剪辑后的式子为：

$$
\lambda_2^{new,uncut}=\lambda_2^{old}+\frac{y_2(E_2-E_2)}{K}\\
\lambda_1^{old}=\begin{cases}
H,\lambda_2^{new,uncut}>H\\
\lambda_2^{new,uncut},L\leqslant\lambda_2^{new,uncut}\leqslant H\\
L,\lambda_2^{new,uncut}<L
\end{cases}
$$

由此可求出

$$
⁍
$$

下面我们对b进行更新

如果$0\leqslant\lambda_1^{new}\leqslant C$可知：

$$
⁍
$$

由此b可以更新为

$$
⁍
$$

写成带损失函数$E(x_i)$的形式

$$
b_1^{new}=-E(X_1)-y_1k_{11}(\lambda_1^{new}-\lambda_1^{old})-y_2k_{21}(\lambda_2^{new}-\lambda_2^{old})+b_1^{old}
$$

同样的，如果有$0\leqslant\lambda_2^{new}\leqslant C$，可知

$$
⁍
$$

用更鲁棒性的更新策略b可以更新如下

$$
⁍
$$

以上就是SMO算法的全部推导过程，是SVM部分的代码核心部分，可以解出上面式子的最大值。

# 拉格朗日对偶化

先po个视频，等我有时间再来补QAQ

[“拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1HP4y1Y79e/)

# 神经网络基础

（主要看了西瓜书的神经网络部分感觉有些模型讲的不是很细，我有时间来补这块的笔记QAQ）

# CNN算法

CNN算法的主要应用领域在特征提取和图像的识别过程，现在有很多深度学习库可以很简单的进行cnn学习，但我们还是有必要了解一下cnn的具体理论。其中反向传播算法我们会单独提出来讲

CNN算法主要由输入层卷积层池化层全连接层和输出层组成，其中卷积和池化可以进行很多次，下面以mnist数据集为例讲讲这些层的具体作用和数学推导。

## 输入层

## 卷积层

## 池化层

## 全连接层

## 输出层

## 用CNN训练mnist手写数据集

1. 定义超参数
2. 构建transform，主要是对图像做变换
3. 加载数据集
4. 构建网络模型
5. 定义训练方法
6. 定义测试方法
7. 开始训练模型

而反向传播算法需要从损失函数层开始反向递推，直到最底层的输入层，计算每一层的梯度。

# 反向传播算法